# AshReports Code Review Fixes - Implementation Plan

## ðŸ“‹ Overview

This plan implements comprehensive fixes for all issues identified in the October 2025 code review. The review identified critical test failures, security vulnerabilities, code duplication (20-25%), and missing test coverage for core renderers.

**Current Status**: Production-capable with critical gaps (Grade: B+)

**Target Status**: Production-ready with comprehensive testing and security hardening (Grade: A)

**Key Findings**:
- ðŸš¨ **Test Suite**: 40-50% coverage, many tests failing
- ðŸš¨ **Security**: 2 high-severity vulnerabilities (atom exhaustion, process dictionary)
- ðŸš¨ **Renderers**: 0% test coverage for PDF/JSON/Interactive
- âš ï¸ **Code Duplication**: ~1,000 lines of duplicate code
- âš ï¸ **Documentation**: Features documented but not implemented

**Review Document**: `notes/comprehensive_code_review_2025-10-04.md`

---

# Stage 1: Critical Blockers (Week 1)

**Duration**: 1 week (30-40 hours)
**Status**: âœ… COMPLETE
**Completed**: 2025-10-07
**Goal**: Fix blocking issues preventing production deployment
**Priority**: CRITICAL - Must complete before any other work

## 1.1 Broken Test Suite Fixes

### 1.1.1 Fix Test Compilation Errors
**Duration**: 4-6 hours
**Files**:
- `test/ash_reports/data_loader/pipeline_test.exs`
- `test/ash_reports/live_view/chart_live_component_test.exs`
- `test/ash_reports/live_view/accessibility_test.exs`

**Tasks**:
- [ ] Fix function arity mismatches in `pipeline_test.exs`
  - Current: `with_mock_data_stream/2` and `with_mock_stream/2` called with 2 args
  - Actual: Functions defined with arity 1 (lines 385-394)
  - Fix: Update function signatures or call sites
  - Test: Ensure pipeline_test.exs compiles and runs

- [ ] Add LiveView test imports to `chart_live_component_test.exs`
  - Missing: `import Phoenix.LiveViewTest`
  - Missing: Component test helpers
  - Add proper setup for LiveView component testing
  - Test: File compiles without errors

- [ ] Add LiveView test imports to `accessibility_test.exs`
  - Same issue as chart_live_component_test.exs
  - Add required imports and helpers
  - Test: File compiles without errors

**Success Criteria**:
- All 3 test files compile without errors
- Test suite can be executed fully
- No blocking compilation errors

### 1.1.2 Fix Struct Definition Mismatches
**Duration**: 2-3 hours
**Files**: Multiple test files expecting old Label struct format

**Tasks**:
- [ ] Update Label struct tests to use `:position` map
  - File: `test/ash_reports/heex_renderer/helpers_test.exs:17`
  - Current: `%Label{name: :test_label, x: 10, y: 20}`
  - Fix: `%Label{name: :test_label, position: %{x: 10, y: 20}}`
  - Search for all Label instantiations in tests
  - Update ~50+ test cases

- [ ] Update Field, Box, Line element tests similarly
  - Check all element test files for position/style patterns
  - Ensure consistency with actual struct definitions
  - Test: Element tests pass

**Success Criteria**:
- All element tests use correct struct format
- No more KeyError exceptions for missing fields
- Element test pass rate improves significantly

### 1.1.3 Add Missing Dependencies
**Duration**: 1-2 hours
**Files**: `mix.exs`

**Tasks**:
- [ ] Add `:statistics` dependency
  - Research: Check if `:statistics` is Erlang library or Hex package
  - Add to mix.exs dependencies
  - Current warnings in: `lib/ash_reports/charts/statistics.ex:134, 208, 210, 253, 255`
  - Alternative: Implement percentile functions locally if library unavailable
  - Test: `mix deps.get` succeeds, statistics module compiles

- [ ] Verify Timex dependency version
  - Current warning: `Timex.week_of_year/1 is undefined`
  - File: `lib/ash_reports/charts/time_series.ex:309`
  - Check Timex version compatibility
  - Update or fix function call
  - Test: No compiler warnings for Timex

**Success Criteria**:
- All dependencies resolve correctly
- No compiler warnings about missing functions
- Statistics functionality works

### 1.1.4 Fix DSL Test Infrastructure
**Duration**: 8-10 hours
**Files**:
- `test/ash_reports/dsl_test.exs` (36/36 failing)
- `test/ash_reports/entities/*_test.exs` (135/135 failing)
- `test/support/test_helpers.ex` (DSL parsing utilities)

**Tasks**:
- [ ] Replace `Code.eval_string` approach
  - Current issue: Causes deadlocks and race conditions
  - Current: `parse_dsl/2` uses dynamic module compilation
  - Solution: Use Spark testing utilities directly
  - Research Spark.Test helpers
  - Implement safer DSL parsing for tests

- [ ] Fix module compilation conflicts
  - Issue: Temporary modules conflict in concurrent tests
  - Use unique module names with better isolation
  - Add proper cleanup between tests
  - Test: DSL tests run reliably

- [ ] Update for Spark API compatibility
  - Review current Spark version vs. test expectations
  - Update DSL entity instantiation in tests
  - Fix any API changes
  - Test: DSL tests pass

**Success Criteria**:
- DSL test pass rate improves from 0% to >80%
- No deadlocks or race conditions
- Tests can run with `async: true` where appropriate

---

## 1.2 Security Vulnerability Patches

**Status**: âœ… COMPLETE
**Completed**: 2025-10-06
**Duration**: 3 hours

### 1.2.1 Fix Atom Creation Security Issue
**Duration**: 2-4 hours
**Priority**: CRITICAL - DoS vulnerability
**Status**: âœ… COMPLETE

**Files**:
- `lib/ash_reports/charts/aggregator.ex:395`
- `lib/ash_reports/json_renderer/chart_api.ex` (lines 223, 380, 389, 392, 431)
- `lib/ash_reports/heex_renderer/live_view_integration.ex` (lines 134, 363, 377)
- **Total**: 11 occurrences across 4 files

**Tasks**:
- [x] Fix charts/aggregator.ex atom creation
  ```elixir
  # Current (UNSAFE):
  defp group_key_name(field) when is_binary(field) do
    String.to_existing_atom(field)
  rescue
    ArgumentError -> String.to_atom(field)  # â† REMOVE THIS
  end

  # Fixed:
  defp group_key_name(field) when is_binary(field), do: field
  defp group_key_name(field) when is_atom(field), do: field
  ```
  - Remove String.to_atom fallback
  - Keep fields as strings if atom doesn't exist
  - Test: Aggregation with string field names works

- [x] Fix json_renderer/chart_api.ex (5 locations)
  - Lines 223, 380, 389, 392, 431
  - Remove all String.to_atom calls on user input
  - Validate against whitelist or keep as strings
  - Test: Chart API works with string keys

- [x] Fix heex_renderer/live_view_integration.ex (3 locations)
  - Lines 134, 363, 377
  - Sanitize user input before atom conversion
  - Use String.to_existing_atom only
  - Test: LiveView integration works safely

- [x] Create validation whitelist for allowed atoms
  - Created `lib/ash_reports/security/atom_validator.ex`
  - Define allowed chart types: [:bar, :line, :pie, :area, :scatter]
  - Define allowed export formats: [:json, :csv, :png, :svg, :pdf, :html]
  - Define allowed chart providers: [:chartjs, :d3, :plotly, :contex]
  - Define allowed aggregation types: [:sum, :count, :avg, :min, :max, :median]
  - Define allowed sort directions: [:asc, :desc]
  - Reject unknown values early with clear errors
  - Test: Invalid types rejected gracefully

**Success Criteria**: âœ… ALL MET
- Zero String.to_atom calls on user-controlled input
- All dynamic atom creation uses whitelist validation via AtomValidator
- Security test: Cannot exhaust atom table via user input
- Functionality preserved with string-based keys
- All tests passing (75 tests in dsl_test.exs and entities/)

### 1.2.2 Document Security Fix
**Duration**: 1 hour
**Files**: Create `SECURITY.md`
**Status**: âœ… COMPLETE

**Tasks**:
- [x] Document atom table exhaustion mitigation
  - Explain the vulnerability
  - Document the fix approach
  - List safe vs unsafe patterns

- [x] Create security testing guide
  - How to test for atom exhaustion
  - Security review checklist
  - Included in SECURITY.md

- [x] Add security section to SECURITY.md
  - Security best practices
  - Code review security focus areas
  - Safe coding practices with examples

**Success Criteria**: âœ… ALL MET
- SECURITY.md created and comprehensive
- Atom table exhaustion vulnerability documented
- Safe vs unsafe patterns clearly explained
- Security testing examples provided
- Security review checklist included
- Future contributors understand security considerations

---

## 1.3 Implementation Status Documentation

**Status**: âœ… COMPLETE
**Completed**: 2025-10-07
**Duration**: 2 hours

### 1.3.1 Create IMPLEMENTATION_STATUS.md
**Duration**: 2-3 hours
**Files**: Create `IMPLEMENTATION_STATUS.md`
**Status**: âœ… COMPLETE

**Tasks**:
- [x] Document implemented features
  - Core DSL features (âœ… Complete)
  - Band-based reporting (âœ… Complete)
  - Chart generation (âœ… Complete)
  - Streaming pipeline (âœ… Complete)
  - PDF rendering (âš ï¸ Untested)
  - JSON rendering (âš ï¸ Untested)

- [x] Mark documented-but-unimplemented features
  - Streaming configuration DSL (âŒ Not Implemented)
  - Security DSL (âŒ Not Implemented)
  - Monitoring DSL (âŒ Not Implemented)
  - Cache configuration DSL (âŒ Not Implemented)

- [x] Add implementation roadmap
  - Stage 2-6 priorities
  - Estimated timelines
  - Dependencies

- [x] Update user guides with implementation notes
  - Add status badges to guide sections
  - Link to IMPLEMENTATION_STATUS.md
  - Clarify what's production-ready vs planned

**Success Criteria**: âœ… ALL MET
- Users can clearly see what's implemented
- No confusion about missing features
- Roadmap provides clear path forward
- Comprehensive feature matrix created
- "What Works Right Now" section provides clear guidance

### 1.3.2 Update README.md
**Duration**: 2-3 hours
**Files**: `README.md`
**Status**: âœ… COMPLETE

**Tasks**:
- [x] Expand README from 60 lines to ~250 lines (expanded to 471 lines)
  - Add project description and goals
  - Add feature matrix with implementation status
  - Add quick start example (5 minutes)
  - Link to comprehensive guides
  - Add installation and setup instructions
  - Add troubleshooting section
  - Add links to community/support

- [x] Create feature matrix table
  ```markdown
  | Feature | Status | Docs | Tests |
  |---------|--------|------|-------|
  | DSL Definition | âœ… Complete | âœ… | âœ… |
  | Band Rendering | âœ… Complete | âœ… | âœ… |
  | Charts | âœ… Complete | âœ… | âœ… |
  | PDF Export | âš ï¸ Untested | âœ… | âŒ |
  ```

- [x] Add quick start code example
  - Simple report definition
  - Data generation
  - Report execution
  - Output formats

**Success Criteria**: âœ… ALL MET
- README serves as proper entry point
- New users can get started quickly
- Feature status clear at a glance
- README expanded from 60 to 471 lines
- Comprehensive quick start with working code examples
- Clear feature status badges and links

---

# Stage 2: Test Infrastructure & Coverage (Weeks 2-3)

**Duration**: 2-3 weeks (80-120 hours)
**Status**: ðŸ“‹ Planned
**Goal**: Achieve >70% test coverage and fix all broken tests
**Priority**: HIGH - Required for production confidence

## 2.1 Test Infrastructure Improvements

**Status**: âœ… COMPLETE
**Completed**: 2025-10-07
**Duration**: 2 hours

### 2.1.1 LiveView Test Infrastructure Setup
**Duration**: 1 day (6-8 hours)
**Files**:
- `test/support/conn_case.ex` (updated)
- `test/support/live_view_test_helpers.ex` (created)
**Status**: âœ… COMPLETE

**Tasks**:
- [x] Create ConnCase module for LiveView testing
  ```elixir
  defmodule AshReportsWeb.ConnCase do
    use ExUnit.CaseTemplate

    using do
      quote do
        import Plug.Conn
        import Phoenix.ConnTest
        import Phoenix.LiveViewTest
        alias AshReportsWeb.Router.Helpers, as: Routes
      end
    end

    setup tags do
      # Setup test endpoint, routes, etc.
    end
  end
  ```

- [x] Implement `live_isolated_component/2` helper
  - Wrap Phoenix.LiveViewTest.live_isolated/3
  - Add component-specific setup
  - Handle async vs sync properly

- [x] Create test endpoint configuration
  - Configure routes for testing
  - Setup proper session handling
  - Add test-specific LiveView configuration

- [x] Add component test helpers
  - Helper for rendering components
  - Helper for sending updates
  - Helper for asserting renders

**Success Criteria**: âœ… ALL MET
- LiveView tests compile successfully
- Component isolation works correctly
- Tests can interact with LiveView properly
- 15 helper functions created in LiveViewTestHelpers module
- ConnCase updated with LiveView and session support

### 2.1.2 Fix DSL Testing Utilities
**Duration**: 1-2 days (covered in 1.1.4)
**Note**: Already covered in Stage 1.1.4
**Status**: âœ… COMPLETE (Section 1.1.4)

### 2.1.3 Create Renderer Test Helpers
**Duration**: 1 day (6-8 hours)
**Files**: `test/support/renderer_test_helpers.ex` (created)
**Status**: âœ… COMPLETE

**Tasks**:
- [x] Create mock RenderContext builder
  - Helper to build valid RenderContext
  - Factory for different report types
  - Configurable data and options

- [x] Add renderer assertion helpers
  - Assert PDF structure/content
  - Assert JSON schema compliance
  - Assert HTML structure
  - Extract and validate rendered output

- [x] Create renderer stub modules
  - Stub for external dependencies (ChromicPDF)
  - Mock chart generation
  - Mock data loading

**Success Criteria**: âœ… ALL MET
- Easy to test any renderer
- Consistent test patterns across renderers
- Can test without external dependencies
- 30+ helper functions created for HTML, PDF, JSON testing
- Mock builders for reports, data, charts
- Performance measurement helpers included

---

## 2.2 PDF Renderer Test Coverage

### 2.2.1 PDF Generator Core Tests
**Duration**: 2 days (12-16 hours)
**Files**: Create `test/ash_reports/pdf_renderer/pdf_generator_test.exs`

**Untested Modules**:
- `lib/ash_reports/pdf_renderer/pdf_generator.ex`
- `lib/ash_reports/pdf_renderer/chart_image_generator.ex`
- `lib/ash_reports/pdf_renderer/page_manager.ex`

**Tasks**:
- [ ] Test PDF generation happy path
  - Simple report to PDF
  - Verify PDF file created
  - Validate PDF structure
  - Check metadata

- [ ] Test chart image generation
  - Charts render to images
  - Image embedding in PDF
  - Image dimensions correct
  - Multiple charts per page

- [ ] Test page management
  - Page breaks work correctly
  - Header/footer rendering
  - Multi-page reports
  - Page numbering

- [ ] Test error scenarios
  - Invalid template handling
  - Chrome launch failure
  - Memory limit exceeded
  - Timeout scenarios

**Success Criteria**:
- 20+ PDF generator tests
- Core PDF functionality validated
- Error handling tested
- >70% code coverage for PDF modules

### 2.2.2 PDF Session and Cleanup Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create test files for session management and cleanup

**Untested Modules**:
- `lib/ash_reports/pdf_renderer/pdf_session_manager.ex`
- `lib/ash_reports/pdf_renderer/temp_file_cleanup.ex`
- `lib/ash_reports/pdf_renderer/print_optimizer.ex`

**Tasks**:
- [ ] Test session lifecycle
  - Session creation
  - Session reuse
  - Session cleanup
  - Concurrent sessions

- [ ] Test temp file cleanup
  - Files cleaned up after generation
  - Cleanup on error
  - Cleanup on timeout
  - No orphaned temp files

- [ ] Test print optimization
  - Pagination optimization
  - Image compression
  - CSS optimization
  - Memory usage optimization

**Success Criteria**:
- No memory leaks in tests
- Temp files always cleaned up
- Session management reliable
- Optimization features tested

### 2.2.3 PDF Template Adapter Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create `test/ash_reports/pdf_renderer/template_adapter_test.exs`

**Untested Module**:
- `lib/ash_reports/pdf_renderer/template_adapter.ex`

**Tasks**:
- [ ] Test HTML template conversion
  - Report DSL â†’ HTML template
  - Band rendering in HTML
  - Element rendering
  - Styling application

- [ ] Test template data binding
  - Variable substitution
  - Expression evaluation
  - Conditional rendering
  - Iteration/loops

- [ ] Test error handling
  - Invalid template syntax
  - Missing data
  - Expression errors

**Success Criteria**:
- Template adapter fully tested
- All HTML conversion paths validated
- Error cases handled gracefully

---

## 2.3 JSON Renderer Test Coverage

### 2.3.1 JSON Core Renderer Tests
**Duration**: 2 days (12-16 hours)
**Files**: Create JSON renderer test suite

**Untested Modules**:
- `lib/ash_reports/json_renderer/structure_builder.ex`
- `lib/ash_reports/json_renderer/data_serializer.ex`
- `lib/ash_reports/json_renderer/schema_manager.ex`

**Tasks**:
- [ ] Test JSON structure building
  - Report structure to JSON
  - Nested bands
  - Element types
  - Metadata inclusion

- [ ] Test data serialization
  - Type conversion (DateTime, Decimal, Money)
  - Relationship handling
  - Circular reference detection
  - Large dataset serialization

- [ ] Test schema management
  - JSON schema generation
  - Schema validation
  - Versioning
  - Documentation

**Success Criteria**:
- 15+ JSON renderer tests
- All data types serialize correctly
- Schema validation works
- >70% code coverage

### 2.3.2 JSON Chart API Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create `test/ash_reports/json_renderer/chart_api_test.exs`

**Untested Module**:
- `lib/ash_reports/json_renderer/chart_api.ex`

**Tasks**:
- [ ] Test chart data export to JSON
  - Chart configuration
  - Chart data format
  - Multiple charts
  - Chart types

- [ ] Test API endpoints (if applicable)
  - Request validation
  - Response format
  - Error responses
  - Security (covered in Stage 1)

**Success Criteria**:
- Chart JSON API fully tested
- API contract validated
- Security fixes verified (from Stage 1)

### 2.3.3 JSON Streaming Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create `test/ash_reports/json_renderer/streaming_engine_test.exs`

**Untested Module**:
- `lib/ash_reports/json_renderer/streaming_engine.ex`

**Tasks**:
- [ ] Test JSON streaming for large reports
  - Chunked JSON output
  - Backpressure handling
  - Memory efficiency
  - Error mid-stream

- [ ] Test NDJSON format (if supported)
  - Newline-delimited JSON
  - Streaming friendly format

**Success Criteria**:
- Streaming tests pass
- Memory usage validated
- Large dataset handling confirmed

---

## 2.4 Interactive Engine Test Coverage

### 2.4.1 Interactive Engine Tests
**Duration**: 1.5 days (10-12 hours)
**Files**: Create test suite for interactive features

**Untested Modules**:
- `lib/ash_reports/interactive_engine/filter_processor.ex`
- `lib/ash_reports/interactive_engine/pivot_processor.ex`
- `lib/ash_reports/interactive_engine/statistical_analyzer.ex`

**Tasks**:
- [ ] Test filter processor
  - Dynamic filter application
  - Filter validation
  - Multiple filters
  - Complex filter expressions

- [ ] Test pivot processor
  - Data pivoting
  - Aggregation
  - Multi-dimensional pivots
  - Edge cases (empty data, single row)

- [ ] Test statistical analyzer
  - Summary statistics
  - Distribution analysis
  - Correlation
  - Outlier detection

**Success Criteria**:
- Interactive features fully tested
- User interaction scenarios validated
- Statistical calculations verified

---

## 2.5 Security Hardening (Process Dictionary Replacement)

### 2.5.1 Replace Format Spec Registry
**Duration**: 1 day (6-8 hours)
**Files**: `lib/ash_reports/formatter.ex`

**Current Issue**: Process dictionary used for format spec registry (lines 715-735)

**Tasks**:
- [ ] Create FormatSpecRegistry GenServer
  ```elixir
  defmodule AshReports.FormatSpecRegistry do
    use Agent

    def start_link(_) do
      Agent.start_link(fn -> %{} end, name: __MODULE__)
    end

    def register(name, spec) do
      Agent.update(__MODULE__, &Map.put(&1, name, spec))
    end

    def get(name) do
      Agent.get(__MODULE__, &Map.get(&1, name))
    end

    def list do
      Agent.get(__MODULE__, & &1)
    end
  end
  ```

- [ ] Update Formatter module to use registry
  - Replace Process.get/1 calls
  - Replace Process.put/2 calls
  - Add registry to supervision tree
  - Test: Formatter works with new registry

- [ ] Add registry to application supervision tree
  - Update `lib/ash_reports/application.ex`
  - Add FormatSpecRegistry as child
  - Test: Registry starts with application

**Success Criteria**:
- No process dictionary usage in Formatter
- Format specs work as before
- Registry accessible across processes
- Tests pass

### 2.5.2 Replace Locale Process Dictionary
**Duration**: 1 day (6-8 hours)
**Files**: `lib/ash_reports/cldr.ex`

**Current Issue**: Locale stored in process dictionary (lines 422, 450, 522)

**Tasks**:
- [ ] Refactor locale passing via context
  - Add locale to RenderContext
  - Pass locale through function arguments
  - Remove Process.get(:ash_reports_locale)
  - Remove Process.put(:ash_reports_locale, locale)

- [ ] Update all locale usage
  - Cldr module functions
  - Formatter locale access
  - Renderer locale handling

- [ ] For web requests, use Plug.Conn assigns
  ```elixir
  # In plug:
  conn = assign(conn, :locale, get_user_locale())

  # In LiveView:
  socket = assign(socket, :locale, get_user_locale())
  ```

**Success Criteria**:
- No locale in process dictionary
- Locale passed explicitly
- Web requests use conn/socket assigns
- Concurrent requests work correctly

### 2.5.3 Replace PDF Session Storage
**Duration**: 1 day (6-8 hours)
**Files**: `lib/ash_reports/pdf_renderer/pdf_generator.ex`

**Current Issue**: Session data in process dictionary (lines 425-450)

**Tasks**:
- [ ] Create PDFSessionRegistry with ETS
  ```elixir
  defmodule AshReports.PDFRenderer.SessionRegistry do
    use GenServer

    def start_link(_) do
      GenServer.start_link(__MODULE__, :ok, name: __MODULE__)
    end

    def init(:ok) do
      :ets.new(:pdf_sessions, [:named_table, :public, read_concurrency: true])
      {:ok, %{}}
    end

    def create_session(session_id, session_data) do
      :ets.insert(:pdf_sessions, {session_id, session_data})
    end

    def get_session(session_id) do
      case :ets.lookup(:pdf_sessions, session_id) do
        [{^session_id, session_data}] -> {:ok, session_data}
        [] -> {:error, :not_found}
      end
    end

    def delete_session(session_id) do
      :ets.delete(:pdf_sessions, session_id)
    end
  end
  ```

- [ ] Update PDF generator to use registry
  - Replace Process.put/get calls
  - Add session cleanup
  - Add session timeout

**Success Criteria**:
- No session data in process dictionary
- Sessions isolated per request
- Proper cleanup on completion/error

### 2.5.4 Remove Remaining Process Dictionary Usage
**Duration**: 1 day (6-8 hours)
**Files**:
- `lib/ash_reports/json_renderer/data_serializer.ex`
- `lib/ash_reports/render_context.ex`

**Tasks**:
- [ ] Audit all Process.get/put calls
  - Search codebase: `Process.get`
  - Search codebase: `Process.put`
  - Document each usage
  - Plan refactoring

- [ ] Refactor or justify each usage
  - Remove if possible
  - Pass through context if needed
  - Use GenServer/ETS if state needed
  - Document if truly necessary (rare)

**Success Criteria**:
- <5 total Process.get/put calls remaining
- All remaining uses documented and justified
- No hidden state issues

---

# Stage 3: Code Quality & Refactoring (Weeks 4-5)

**Duration**: 2-3 weeks (80-120 hours)
**Status**: ðŸ“‹ Planned
**Goal**: Reduce code duplication from 20-25% to <10%
**Priority**: MEDIUM-HIGH - Improves maintainability significantly

## 3.1 Chart Integration Deduplication

### 3.1.1 Extract AshReports.ChartIntegration Module
**Duration**: 3-4 days (20-30 hours)
**Files**: Create `lib/ash_reports/chart_integration.ex`

**Duplicate Code** (~400 lines across 4 files):
- `lib/ash_reports/html_renderer.ex` (lines 666-856)
- `lib/ash_reports/heex_renderer.ex` (lines 538-717)
- `lib/ash_reports/json_renderer.ex` (lines 565-646)
- `lib/ash_reports/pdf_renderer.ex` (lines 454-524)

**Tasks**:
- [ ] Create ChartIntegration module
  ```elixir
  defmodule AshReports.ChartIntegration do
    @moduledoc """
    Shared chart integration logic for all renderers.
    Consolidates chart configuration extraction, processing,
    and rendering across HTML, HEEX, JSON, and PDF renderers.
    """

    def extract_charts(context)
    def generate_chart_id(config)
    def prepare_chart_requirements(context)
    def process_chart_config(config)

    # Renderer-specific adapters
    def for_html(context, charts)
    def for_json(context, charts)
    def for_pdf(context, charts)
    def for_heex(context, charts)
  end
  ```

- [ ] Extract common chart functions
  - `extract_chart_configs_from_context/1`
  - `process_chart_requirements/1`
  - `generate_chart_id/1`
  - `prepare_chart_data/2`
  - `validate_chart_config/1`

- [ ] Create renderer adapters
  - HTML adapter for chart HTML generation
  - JSON adapter for chart JSON structure
  - PDF adapter for chart image generation
  - HEEX adapter for LiveView integration

- [ ] Update all renderers to use ChartIntegration
  - Replace duplicate code with module calls
  - Pass renderer type to adapter
  - Test each renderer

**Success Criteria**:
- ~400 lines of duplicate code eliminated
- Single source of truth for chart logic
- All renderers work as before
- Tests pass

### 3.1.2 Add ChartIntegration Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create `test/ash_reports/chart_integration_test.exs`

**Tasks**:
- [ ] Test chart extraction
- [ ] Test chart ID generation
- [ ] Test each renderer adapter
- [ ] Test error handling

**Success Criteria**:
- ChartIntegration module fully tested
- >80% coverage

---

## 3.2 Renderer Base Deduplication

### 3.2.1 Create AshReports.Renderer.Base Module
**Duration**: 2-3 days (12-20 hours)
**Files**: Create `lib/ash_reports/renderer/base.ex`

**Duplicate Code** (~300 lines across 4 files):
- All renderer modules have nearly identical:
  - `render_with_context/2` structure
  - `validate_context/1` pattern
  - `prepare/2` pattern
  - Metadata building
  - Error handling

**Tasks**:
- [ ] Create Renderer.Base with __using__ macro
  ```elixir
  defmodule AshReports.Renderer.Base do
    @moduledoc """
    Base module for renderer implementations.
    Provides common render pipeline and utilities.
    """

    @callback do_render(RenderContext.t(), Keyword.t()) ::
      {:ok, rendered()} | {:error, term()}

    @callback renderer_specific_validation(RenderContext.t()) ::
      :ok | {:error, term()}

    defmacro __using__(opts) do
      quote do
        @behaviour AshReports.Renderer

        def render_with_context(context, opts \\ []) do
          AshReports.Renderer.Base.render_with_timing(
            __MODULE__,
            context,
            opts
          )
        end

        def validate_context(context) do
          with :ok <- AshReports.Renderer.Base.common_validation(context),
               :ok <- renderer_specific_validation(context) do
            :ok
          end
        end

        def prepare(context, opts) do
          AshReports.Renderer.Base.prepare_context(
            __MODULE__,
            context,
            opts
          )
        end

        # Default implementations (can be overridden)
        def renderer_specific_validation(_context), do: :ok
        def supports_streaming?, do: false

        defoverridable [
          renderer_specific_validation: 1,
          supports_streaming: 0
        ]
      end
    end

    # Shared implementations
    def render_with_timing(module, context, opts)
    def common_validation(context)
    def prepare_context(module, context, opts)
    def build_base_metadata(context, start_time)
  end
  ```

- [ ] Update all renderers to use Renderer.Base
  - `use AshReports.Renderer.Base`
  - Implement `do_render/2` callback
  - Implement `renderer_specific_validation/1`
  - Remove duplicate code

- [ ] Test each renderer
  - Verify behavior unchanged
  - Check error handling
  - Validate metadata

**Success Criteria**:
- ~300 lines of duplicate code eliminated
- All renderers use common base
- Tests pass
- Behavior unchanged

### 3.2.2 Add Renderer.Base Tests
**Duration**: 1 day (6-8 hours)
**Files**: Create `test/ash_reports/renderer/base_test.exs`

**Tasks**:
- [ ] Test common validation
- [ ] Test timing/metadata
- [ ] Test error handling
- [ ] Test with each renderer

**Success Criteria**:
- Base module fully tested
- Renderer integration verified

---

## 3.3 Validation Utilities Deduplication

### 3.3.1 Create AshReports.Validation Module
**Duration**: 2 days (12-16 hours)
**Files**: Create `lib/ash_reports/validation.ex`

**Duplicate Code** (~200 lines across 42 files):
- Validation functions scattered across:
  - Verifiers
  - Renderers
  - Data loaders
  - Engines

**Tasks**:
- [ ] Create Validation utilities module
  ```elixir
  defmodule AshReports.Validation do
    @moduledoc """
    Common validation utilities for AshReports.
    """

    def validate_presence(value, field_name)
    def validate_type(value, expected_type, field_name)
    def validate_all(validations)
    def validate_required_fields(struct, field_names)
    def validate_one_of(value, allowed, field_name)
    def validate_format(value, regex, field_name)
  end
  ```

- [ ] Extract common validation patterns
  - Presence validation
  - Type validation
  - Format validation
  - Required fields
  - Allowed values

- [ ] Update modules to use Validation
  - Replace duplicate validation code
  - Use common utilities
  - Standardize error formats

**Success Criteria**:
- ~200 lines duplicate validation eliminated
- Consistent validation across codebase
- Standard error format

### 3.3.2 Standardize Error Tuples
**Duration**: 1 day (6-8 hours)

**Current Issue**: Inconsistent error tuple formats

**Tasks**:
- [ ] Define standard error structure
  ```elixir
  @type error_category :: :validation | :rendering | :data_loading | :configuration
  @type error :: {:error, {error_category(), atom(), String.t()}}
  ```

- [ ] Create AshReports.Error module
  ```elixir
  defmodule AshReports.Error do
    def validation_error(code, message)
    def rendering_error(stage, message)
    def data_loading_error(reason)
    def configuration_error(field, message)
  end
  ```

- [ ] Update error returns throughout codebase
  - Search for `{:error, ` patterns
  - Standardize to new format
  - Update error handling

**Success Criteria**:
- Consistent error format
- Clear error categorization
- Better error messages

---

## 3.4 Metadata Building Deduplication

### 3.4.1 Create AshReports.MetadataBuilder Module
**Duration**: 1 day (6-8 hours)
**Files**: Create `lib/ash_reports/metadata_builder.ex`

**Duplicate Code** (~100 lines across 4 files):
- `lib/ash_reports/html_renderer.ex` (lines 455-486)
- `lib/ash_reports/json_renderer.ex` (lines 436-468)
- `lib/ash_reports/pdf_renderer.ex` (lines 307-336)

**Tasks**:
- [ ] Create MetadataBuilder module
  ```elixir
  defmodule AshReports.MetadataBuilder do
    def build(context, start_time, format_specific_data)
    def base_metadata(context, start_time)
    def add_render_timing(metadata, start_time)
    def add_locale_info(metadata, context)
  end
  ```

- [ ] Extract common metadata fields
  - Render time calculation
  - Locale information
  - Text direction
  - Format type
  - Timestamp

- [ ] Update renderers to use builder
  - Replace metadata building code
  - Pass format-specific data
  - Test metadata structure

**Success Criteria**:
- ~100 lines duplicate code eliminated
- Consistent metadata structure
- All renderers updated

---

## 3.5 Element Module Standardization

### 3.5.1 Create Element Module Standard Pattern
**Duration**: 2 days (12-16 hours)
**Files**: All element modules in `lib/ash_reports/element/`

**Current Issue**: Inconsistent element module structure
- Some have `process_options/1`, some don't
- Chart missing `process_options/1`
- Inconsistent constructor patterns

**Tasks**:
- [ ] Create Element base module with `__using__` macro
  ```elixir
  defmodule AshReports.Element do
    defmacro __using__(element_type) do
      quote do
        defstruct [:name, :type, :position, :style, :conditional | unquote(element_specific_fields())]

        @type t :: %__MODULE__{...}

        def new(name, opts \\ []) do
          AshReports.Element.build_element(
            __MODULE__,
            unquote(element_type),
            name,
            opts
          )
        end

        defp process_options(opts) do
          AshReports.Element.process_common_options(opts)
          |> process_element_specific_options()
        end

        # Override in each element
        defp process_element_specific_options(opts), do: opts

        defoverridable [process_element_specific_options: 1]
      end
    end

    def build_element(module, type, name, opts)
    def process_common_options(opts)
  end
  ```

- [ ] Update all element modules to use base
  - Field, Label, Box, Line, Image, Chart
  - Use `use AshReports.Element, :element_type`
  - Add element-specific processing
  - Remove duplicate code

- [ ] Add Chart module `process_options/1`
  - Bring Chart to parity with other elements
  - Process position and style
  - Test Chart with standard pattern

**Success Criteria**:
- ~60 lines duplicate code eliminated
- All elements follow same pattern
- Easy to add new element types
- Tests pass

---

## 3.6 Consistency Improvements

### 3.6.1 Standardize Test File Naming
**Duration**: 4 hours
**Files**: Rename test files

**Current Issue**: Mixed test naming patterns

**Tasks**:
- [ ] Document standard naming convention
  ```
  test/ash_reports/{module_path}_test.exs

  Examples:
  test/ash_reports/entities/report_test.exs
  test/ash_reports/data_loader/pipeline_test.exs
  test/ash_reports/renderers/html_renderer_test.exs
  test/ash_reports/integration/phase_{n}_{name}_test.exs
  ```

- [ ] Rename inconsistent test files
  - Update file paths
  - Update test module names
  - Update any references
  - Run tests to verify

**Success Criteria**:
- Consistent test file naming
- Easy to find tests for any module

### 3.6.2 Standardize Configuration Patterns
**Duration**: 1 day (6-8 hours)
**Files**: Renderer configuration handling

**Current Issue**: Inconsistent config patterns across renderers

**Tasks**:
- [ ] Create AshReports.Config module
  ```elixir
  defmodule AshReports.Config do
    def renderer_config(type, opts)
    def engine_config(type, opts)
    def validate_config(config)
    def merge_configs(base, override)
  end
  ```

- [ ] Standardize renderer config structure
  - All use nested maps with renderer key
  - Consistent option names
  - Standard defaults

- [ ] Update renderers to use Config
  - Remove duplicate config handling
  - Use standard structure
  - Test configuration

**Success Criteria**:
- Consistent config across renderers
- Single config validation
- Tests pass

---

# Stage 4: Architecture Improvements (Month 2)

**Duration**: 3-4 weeks (120-160 hours)
**Status**: ðŸ“‹ Planned
**Goal**: Improve architecture for long-term maintainability
**Priority**: MEDIUM - Important for future development

## 4.1 TemplateEngine Abstraction

### 4.1.1 Extract TemplateEngine Behavior
**Duration**: 1 week (30-40 hours)
**Files**: Create `lib/ash_reports/template_engine.ex`

**Current Issue**: Typst tightly coupled throughout system

**Tasks**:
- [ ] Define TemplateEngine behavior
  ```elixir
  defmodule AshReports.TemplateEngine do
    @moduledoc """
    Behavior for template engines.
    Allows swapping Typst for alternatives (LaTeX, Weasyprint, etc.)
    """

    @callback compile_template(template :: String.t(), context :: map()) ::
      {:ok, binary()} | {:error, term()}

    @callback embed_asset(asset :: binary(), type :: atom(), opts :: keyword()) ::
      {:ok, String.t()} | {:error, term()}

    @callback supports_format?(format :: atom()) :: boolean()
    @callback file_extension() :: String.t()
  end
  ```

- [ ] Create TypstEngine implementation
  ```elixir
  defmodule AshReports.TemplateEngines.Typst do
    @behaviour AshReports.TemplateEngine

    def compile_template(template, context) do
      # Existing Typst compilation logic
    end

    def embed_asset(svg, :svg, opts) do
      # Existing SVG embedding logic
    end

    def supports_format?(:pdf), do: true
    def supports_format?(:png), do: true
    def supports_format?(_), do: false

    def file_extension, do: ".pdf"
  end
  ```

- [ ] Create engine registry and selection
  ```elixir
  defmodule AshReports.TemplateEngineRegistry do
    def register_engine(name, module)
    def get_engine(name)
    def list_engines()
  end
  ```

- [ ] Update ChartPreprocessor to use abstraction
  - Remove direct Typst dependencies
  - Use TemplateEngine behavior
  - Pass engine via configuration

- [ ] Update ChartEmbedder to use abstraction
  - Generic asset embedding
  - Engine-specific formatting

**Success Criteria**:
- Typst decoupled from core
- Can swap engines via configuration
- Tests pass with Typst engine
- Foundation for alternative engines

### 4.1.2 Update Documentation for Engine Abstraction
**Duration**: 1 day (6-8 hours)

**Tasks**:
- [ ] Document TemplateEngine behavior
- [ ] Create guide for adding engines
- [ ] Update architecture documentation

**Success Criteria**:
- Clear documentation for engine abstraction
- Guide for implementing new engines

---

## 4.2 Chart System Consolidation

### 4.2.1 Document ChartEngine vs Charts
**Duration**: 1 day (6-8 hours)
**Files**: Create `docs/charts_architecture.md`

**Current Issue**: Two overlapping chart systems

**ChartEngine**: Multi-provider (ChartJS, D3, Plotly), JSON output
**Charts**: Pure Elixir (Contex), SVG output

**Tasks**:
- [ ] Document each system's purpose
  - ChartEngine: Interactive web charts, JSON API
  - Charts: Report embedded charts, SVG for PDF

- [ ] Document use cases
  - Use Charts for: Report PDFs, Typst documents
  - Use ChartEngine for: Interactive dashboards, web apps

- [ ] Create decision matrix
  - When to use which system
  - Feature comparison
  - Performance characteristics

- [ ] Add implementation status
  - ChartEngine: D3/Plotly status unclear
  - Charts: Fully implemented

**Success Criteria**:
- Clear documentation on which to use when
- No developer confusion
- Use cases well-defined

### 4.2.2 Evaluate Consolidation Options
**Duration**: 2 days (12-16 hours)

**Tasks**:
- [ ] Option 1: Keep both, document clearly
  - Pros: Different use cases
  - Cons: Maintenance overhead

- [ ] Option 2: Consolidate into one
  - Migrate ChartEngine to Charts
  - Add JSON output to Charts
  - Single chart system

- [ ] Option 3: Abstract common interface
  - Create ChartProvider behavior
  - Both implement same interface
  - Easy to swap

- [ ] Document recommendation
- [ ] Create migration plan if consolidating

**Success Criteria**:
- Clear architectural decision
- Plan for implementation
- Timeline and effort estimated

---

## 4.3 Renderer Middleware System

### 4.3.1 Design Middleware Architecture
**Duration**: 2 days (12-16 hours)
**Files**: Create `lib/ash_reports/renderer/middleware.ex`

**Goal**: Enable cross-cutting concerns without coupling

**Tasks**:
- [ ] Define Middleware behavior
  ```elixir
  defmodule AshReports.Renderer.Middleware do
    @callback before_render(context, opts) ::
      {:ok, context} | {:error, term()}

    @callback after_render(context, result) ::
      {:ok, result} | {:error, term()}

    @callback on_error(context, error) ::
      {:ok, result} | {:error, term()}
  end
  ```

- [ ] Create middleware pipeline
  ```elixir
  defmodule AshReports.Renderer.MiddlewarePipeline do
    def run(middlewares, context, renderer_fn)
    def run_before(middlewares, context)
    def run_after(middlewares, context, result)
    def run_error(middlewares, context, error)
  end
  ```

- [ ] Integrate with Renderer.Base
  - Add middleware support to base
  - Configure middleware per renderer
  - Execute pipeline around render

**Success Criteria**:
- Middleware architecture defined
- Integration point clear
- Ready for middleware implementations

### 4.3.2 Implement Common Middleware
**Duration**: 2-3 days (16-24 hours)

**Tasks**:
- [ ] Create Caching middleware
  ```elixir
  defmodule AshReports.Renderer.Middleware.Caching do
    @behaviour AshReports.Renderer.Middleware

    def before_render(context, _opts) do
      case Cache.get(cache_key(context)) do
        nil -> {:ok, context}
        cached -> {:cached, cached}
      end
    end

    def after_render(context, result) do
      Cache.put(cache_key(context), result)
      {:ok, result}
    end
  end
  ```

- [ ] Create Telemetry middleware
  - Emit events before/after render
  - Track render duration
  - Track errors

- [ ] Create AccessControl middleware
  - Check permissions before render
  - Log access
  - Enforce row-level security

- [ ] Create RateLimiting middleware (optional)
  - Limit renders per user
  - Prevent abuse

**Success Criteria**:
- 3+ middleware implemented
- Renderers can use middleware
- Pluggable architecture works

---

## 4.4 Context Contracts and Type Safety

### 4.4.1 Define Explicit Context Contracts
**Duration**: 2 days (12-16 hours)
**Files**: `lib/ash_reports/render_context.ex`

**Current Issue**: Implicit context shape, no validation

**Tasks**:
- [ ] Add @type specifications for context
  ```elixir
  @type t :: %__MODULE__{
    report: Report.t(),
    data: data_result(),
    metadata: metadata(),
    config: config(),
    locale: String.t(),
    # ... all fields explicitly typed
  }
  ```

- [ ] Define typed metadata
  ```elixir
  @type metadata :: %{
    charts: chart_map(),
    variables: variable_map(),
    # ... all metadata fields
  }
  ```

- [ ] Add validation functions
  ```elixir
  def validate_context(context)
  def validate_metadata(metadata)
  def validate_config(config)
  ```

- [ ] Add context builders
  ```elixir
  def new(report, data, opts)
  def put_metadata(context, key, value)
  def merge_config(context, config)
  ```

**Success Criteria**:
- All context fields typed
- Validation functions work
- Dialyzer passes
- Documentation clear

### 4.4.2 Add Dialyzer to CI
**Duration**: 1 day (6-8 hours)

**Tasks**:
- [ ] Add Dialyzer to project
  - Add dialyxir dependency
  - Generate PLT
  - Run dialyzer

- [ ] Fix Dialyzer warnings
  - Type mismatches
  - Missing @spec
  - Incorrect return types

- [ ] Add to CI pipeline
  - Run on every PR
  - Fail on warnings

**Success Criteria**:
- Dialyzer runs successfully
- No warnings
- CI integration working

---

# Stage 5: Documentation & Developer Experience (Months 2-3)

**Duration**: 3-4 weeks (120-160 hours)
**Status**: ðŸ“‹ Planned
**Goal**: Complete documentation and improve developer experience
**Priority**: MEDIUM - Important for adoption and contribution

## 5.1 API Documentation

### 5.1.1 Document Core API
**Duration**: 2 days (12-16 hours)
**Files**: `lib/ash_reports.ex`

**Tasks**:
- [ ] Add comprehensive @doc to generate/4
  ```elixir
  @doc """
  Generates a report with the given parameters.

  ## Parameters

  - `domain` - The Ash domain containing the report definition
  - `report_name` - Atom name of the report to generate
  - `params` - Map of parameters matching report's parameter definitions
  - `format` - Output format (`:html`, `:pdf`, `:heex`, `:json`)

  ## Returns

  - `{:ok, content}` - Successfully generated report content as binary/string
  - `{:error, reason}` - Generation failed with error reason

  ## Examples

      # Generate HTML report
      {:ok, html} = AshReports.generate(MyApp.Domain, :sales_report, %{
        start_date: ~D[2024-01-01],
        end_date: ~D[2024-12-31]
      }, :html)

      # Generate PDF with parameters
      {:ok, pdf} = AshReports.generate(MyApp.Domain, :invoice_report, %{
        invoice_id: "INV-001"
      }, :pdf)

  ## Error Cases

  - `{:error, :report_not_found}` - Report name doesn't exist in domain
  - `{:error, {:validation_failed, errors}}` - Parameter validation failed
  - `{:error, {:query_failed, reason}}` - Database query failed
  - `{:error, {:render_failed, reason}}` - Rendering failed

  ## Performance Considerations

  - Large reports may take significant time and memory
  - Consider using streaming for datasets > 10,000 records
  - PDF generation requires Chrome/Chromium installed
  - Charts are generated server-side and embedded

  ## See Also

  - `AshReports.generate_async/4` - Async generation
  - `AshReports.generate_stream/4` - Streaming generation
  """
  @spec generate(module(), atom(), map(), format()) ::
    {:ok, binary()} | {:error, term()}
  def generate(domain, report_name, params \\ %{}, format \\ :html)
  ```

- [ ] Document all public API functions
  - generate/4
  - generate_async/4 (if exists)
  - generate_stream/4 (if exists)
  - list_reports/1
  - get_report_info/2

- [ ] Add module examples
  - Quick start
  - Common patterns
  - Advanced usage

**Success Criteria**:
- Core API fully documented
- Examples work and test
- Beginner-friendly

### 5.1.2 Generate ExDoc Documentation
**Duration**: 1 day (6-8 hours)

**Tasks**:
- [ ] Configure ExDoc in mix.exs
  ```elixir
  def project do
    [
      # ...
      docs: [
        main: "AshReports",
        extras: ["README.md", "CHANGELOG.md"],
        groups_for_modules: [
          "Core": [AshReports, AshReports.Report],
          "Renderers": [AshReports.HtmlRenderer, ...],
          "Charts": [AshReports.Charts, ...],
          # ...
        ]
      ]
    ]
  end
  ```

- [ ] Generate and review docs
  - `mix docs`
  - Review output
  - Fix formatting issues

- [ ] Publish docs
  - Setup hex.pm docs
  - Configure auto-publish

**Success Criteria**:
- ExDoc generates successfully
- Docs look professional
- Easy to navigate

---

## 5.2 Developer Guides

### 5.2.1 Create CONTRIBUTING.md
**Duration**: 2 days (12-16 hours)
**Files**: Create `CONTRIBUTING.md`

**Tasks**:
- [ ] Document development setup
  - Prerequisites
  - Installation
  - Running tests
  - Running benchmarks

- [ ] Document code style
  - Elixir style guide
  - Naming conventions
  - Documentation requirements
  - Testing requirements

- [ ] Document PR process
  - Branch naming
  - Commit messages
  - PR template
  - Review process

- [ ] Add security guidelines
  - How to report vulnerabilities
  - Security review checklist
  - Safe coding practices

**Success Criteria**:
- CONTRIBUTING.md comprehensive
- New contributors have clear path
- Security considerations documented

### 5.2.2 Create ARCHITECTURE.md
**Duration**: 3 days (20-24 hours)
**Files**: Create `docs/ARCHITECTURE.md`

**Tasks**:
- [ ] Document system architecture
  - High-level overview
  - Module organization
  - Data flow diagrams
  - Component interactions

- [ ] Document DSL system
  - Spark integration
  - Compile-time vs runtime
  - Transformers and verifiers
  - Code generation

- [ ] Document rendering pipeline
  - Request flow
  - Context creation
  - Template generation
  - Output formatting

- [ ] Document streaming architecture
  - GenStage pipeline
  - Backpressure handling
  - Memory management
  - Aggregation strategy

- [ ] Document chart system
  - Chart generation
  - SVG embedding
  - Multiple chart systems
  - Integration points

**Success Criteria**:
- Architecture clearly explained
- Diagrams helpful
- New developers understand system

### 5.2.3 Create Extension Guides
**Duration**: 2 days (12-16 hours)
**Files**: Create guides in `docs/guides/`

**Tasks**:
- [ ] Create "Adding a New Element Type" guide
  - Step-by-step instructions
  - Code examples
  - Testing requirements

- [ ] Create "Adding a New Renderer" guide
  - Implementing Renderer behavior
  - Using Renderer.Base
  - Testing renderer
  - Adding to registry

- [ ] Create "Adding a New Chart Type" guide
  - Implementing chart behavior
  - Data format requirements
  - SVG generation
  - Registration

- [ ] Create "Creating Middleware" guide
  - Middleware behavior
  - Integration points
  - Common patterns

**Success Criteria**:
- Extension guides clear
- Examples work
- Easy to add new features

---

## 5.3 Documentation Alignment

### 5.3.1 Update User Guides with Implementation Status
**Duration**: 2 days (12-16 hours)
**Files**: `guides/user/*.md`

**Tasks**:
- [ ] Add implementation status badges
  ```markdown
  ## Streaming Configuration âš ï¸ PLANNED

  > **Implementation Status**: This feature is documented but not yet
  > implemented. See [IMPLEMENTATION_STATUS.md](../IMPLEMENTATION_STATUS.md)
  > for details.
  ```

- [ ] Update all advanced features
  - Streaming config DSL (PLANNED)
  - Security DSL (PLANNED)
  - Monitoring DSL (PLANNED)
  - Cache config DSL (PLANNED)

- [ ] Mark production-ready features
  - Core DSL (âœ… COMPLETE)
  - Band rendering (âœ… COMPLETE)
  - Charts (âœ… COMPLETE)
  - Streaming (âœ… COMPLETE)

- [ ] Link to IMPLEMENTATION_STATUS.md
  - Reference in each guide
  - Clear roadmap link

**Success Criteria**:
- No confusion about what works
- Clear implementation status
- Users know what's ready

### 5.3.2 Create Troubleshooting Guide
**Duration**: 2 days (12-16 hours)
**Files**: Create `docs/TROUBLESHOOTING.md`

**Tasks**:
- [ ] Common errors and solutions
  - Test failures
  - Compilation errors
  - Runtime errors
  - Performance issues

- [ ] PDF generation issues
  - Chrome/Chromium not found
  - Memory errors
  - Timeout issues
  - Image rendering problems

- [ ] Chart generation issues
  - Chart not rendering
  - SVG embedding fails
  - Data format errors
  - Styling problems

- [ ] Database query issues
  - Slow queries
  - N+1 queries
  - Relationship loading
  - Memory usage

**Success Criteria**:
- Common issues covered
- Solutions clear and tested
- Links to relevant docs

### 5.3.3 Create Examples Directory
**Duration**: 3 days (20-24 hours)
**Files**: Create `examples/` directory

**Tasks**:
- [ ] Create simple_report example
  - Basic report with fields
  - Minimal configuration
  - All output formats

- [ ] Create grouped_report example
  - Multi-level grouping
  - Aggregations
  - Variables

- [ ] Create chart_report example
  - Multiple chart types
  - Chart configuration
  - Data aggregation

- [ ] Create liveview_integration example
  - LiveView setup
  - Real-time updates
  - Interactive features

**Success Criteria**:
- Examples are runnable
- Examples demonstrate features
- Examples well-documented

---

# Stage 6: Performance & Polish (Month 3+)

**Duration**: 3-4 weeks (120-160 hours)
**Status**: ðŸ“‹ Planned
**Goal**: Optimize performance and prepare for production
**Priority**: LOW-MEDIUM - Nice to have, important for scale

## 6.1 Performance Test Suite

### 6.1.1 Create Performance Benchmarks
**Duration**: 1 week (30-40 hours)
**Files**: Create `test/performance/` directory

**Tasks**:
- [ ] Create large dataset benchmarks
  - 10K records
  - 100K records
  - 1M records
  - Memory profiling

- [ ] Create concurrent access benchmarks
  - 10 concurrent renders
  - 100 concurrent renders
  - Throughput measurement

- [ ] Create chart generation benchmarks
  - Simple charts
  - Complex charts
  - Multiple charts
  - Large datasets

- [ ] Create renderer benchmarks
  - HTML renderer
  - PDF renderer
  - JSON renderer
  - Comparison

**Success Criteria**:
- Benchmarks automated
- Performance baselines established
- Regression detection

### 6.1.2 Performance Optimization
**Duration**: 1-2 weeks (40-60 hours)

**Tasks**:
- [ ] Profile bottlenecks
  - Use :fprof
  - Identify slow functions
  - Memory hotspots

- [ ] Optimize data loading
  - Query optimization
  - Relationship preloading
  - Caching strategy

- [ ] Optimize rendering
  - Template compilation
  - Parallel chart generation
  - Lazy loading

- [ ] Optimize streaming
  - Chunk size tuning
  - Backpressure optimization
  - Memory management

**Success Criteria**:
- Performance targets met
- No obvious bottlenecks
- Scales to production loads

---

## 6.2 End-to-End Integration Tests

### 6.2.1 Create Integration Test Suite
**Duration**: 1 week (30-40 hours)
**Files**: Create `test/integration/` directory

**Tasks**:
- [ ] DSL â†’ Template â†’ Output tests
  - Define report in DSL
  - Generate all formats
  - Validate output

- [ ] Streaming â†’ Chart â†’ PDF tests
  - Large dataset
  - Chart aggregation
  - PDF generation
  - Memory validation

- [ ] LiveView â†’ Report â†’ Update tests
  - Interactive report
  - Real-time updates
  - WebSocket communication

- [ ] Multi-format consistency tests
  - Same report, all formats
  - Validate consistency
  - Data integrity

**Success Criteria**:
- End-to-end paths tested
- Integration issues caught
- Confidence in full system

---

## 6.3 Production Hardening

### 6.3.1 Error Recovery and Resilience
**Duration**: 1 week (30-40 hours)

**Tasks**:
- [ ] Add circuit breakers
  - Database query failures
  - External service failures
  - Memory pressure

- [ ] Add retry logic
  - Transient failures
  - Exponential backoff
  - Max retries

- [ ] Add graceful degradation
  - Simplified output on errors
  - Fallback formats
  - Error reporting

- [ ] Add health checks
  - System health endpoint
  - Dependency checks
  - Resource monitoring

**Success Criteria**:
- System resilient to failures
- Graceful error handling
- Health monitoring works

### 6.3.2 Monitoring and Observability
**Duration**: 1 week (30-40 hours)

**Tasks**:
- [ ] Add comprehensive telemetry
  - All major operations
  - Duration metrics
  - Error tracking
  - Resource usage

- [ ] Create dashboards
  - Report generation metrics
  - System health
  - Error rates
  - Performance trends

- [ ] Add alerting
  - Error rate thresholds
  - Performance degradation
  - Resource exhaustion

**Success Criteria**:
- Full observability
- Issues detected quickly
- Dashboards useful

---

# Integration and Success Criteria

## Stage Dependencies

```
Stage 1 (Critical Blockers) â†’ REQUIRED for all other stages
  â†“
Stage 2 (Test Infrastructure) â†’ REQUIRED for Stage 3-6 validation
  â†“
Stage 3 (Code Quality) â†’ Makes Stage 4-6 easier
  â†“
Stage 4 (Architecture) â†’ Long-term maintainability
  â†“
Stage 5 (Documentation) â†’ Enables contributions
  â†“
Stage 6 (Performance) â†’ Production readiness
```

## Overall Success Criteria

### Testing
- [ ] >80% test coverage overall
- [ ] All critical paths tested
- [ ] No failing tests
- [ ] Performance benchmarks passing

### Security
- [ ] No high-severity vulnerabilities
- [ ] Process dictionary eliminated
- [ ] Atom table safe
- [ ] Security documented

### Code Quality
- [ ] Code duplication <10%
- [ ] Consistent patterns
- [ ] Dialyzer clean
- [ ] Formatted and linted

### Documentation
- [ ] API fully documented
- [ ] Developer guides complete
- [ ] Examples work
- [ ] Implementation status clear

### Performance
- [ ] Targets met
- [ ] Scales to production
- [ ] Memory efficient
- [ ] Benchmarks established

### Architecture
- [ ] Typst decoupled
- [ ] Middleware system working
- [ ] Chart systems clear
- [ ] Extension points defined

## Timeline Summary

| Stage | Duration | Critical Path | Can Start |
|-------|----------|---------------|-----------|
| Stage 1 | 1 week | YES | Immediately |
| Stage 2 | 2-3 weeks | YES | After Stage 1 |
| Stage 3 | 2-3 weeks | NO | After Stage 1 |
| Stage 4 | 3-4 weeks | NO | After Stage 2 |
| Stage 5 | 3-4 weeks | NO | After Stage 1 |
| Stage 6 | 3-4 weeks | NO | After Stage 2 |

**Total Duration**: 14-19 weeks (3.5-4.75 months)
**Critical Path**: 3-4 weeks (Stages 1-2)
**Parallel Work Possible**: Stages 3-6 can overlap significantly

## Team Requirements

- **Stage 1**: 1 senior developer (critical fixes)
- **Stage 2**: 1-2 developers (test writing can parallelize)
- **Stage 3**: 1 senior developer (refactoring requires experience)
- **Stage 4**: 1 senior developer (architecture work)
- **Stage 5**: 1 technical writer or developer (documentation)
- **Stage 6**: 1 developer with performance expertise

**Optimal Team**: 2 senior developers + 1 technical writer

## Risk Mitigation

### Stage 1 Risks
- **Risk**: Test fixes more complex than expected
- **Mitigation**: Timebox each fix, escalate if blocked
- **Risk**: DSL test refactor breaks tests
- **Mitigation**: Incremental approach, verify frequently

### Stage 2 Risks
- **Risk**: Renderer tests uncover major bugs
- **Mitigation**: Fix bugs as found, adjust timeline
- **Risk**: Process dictionary replacement breaks functionality
- **Mitigation**: Comprehensive testing, feature flags for rollback

### Stage 3 Risks
- **Risk**: Refactoring introduces regressions
- **Mitigation**: Test-first approach, incremental changes
- **Risk**: Code duplication harder to remove than expected
- **Mitigation**: Start with highest-impact areas, iterate

### Stage 4 Risks
- **Risk**: Abstraction too complex or wrong
- **Mitigation**: Start simple, iterate based on needs
- **Risk**: Typst decoupling affects performance
- **Mitigation**: Benchmark before/after, optimize if needed

### Stage 5 Risks
- **Risk**: Documentation takes longer than expected
- **Mitigation**: Prioritize core docs, iterate
- **Risk**: Examples don't work
- **Mitigation**: Test examples as part of CI

### Stage 6 Risks
- **Risk**: Performance targets not achievable
- **Mitigation**: Set realistic targets, optimize iteratively
- **Risk**: Integration tests flaky
- **Mitigation**: Invest in test reliability, proper cleanup

---

**Next Steps**:
1. **Review and approve plan** with team
2. **Start Stage 1 immediately** - critical blockers
3. **Set up project tracking** - Use this plan as issues/tasks
4. **Establish testing baseline** - Current coverage metrics
5. **Begin Stage 1.1** - Fix test compilation errors

**Plan Maintenance**:
- Update status as stages complete
- Adjust timelines based on actual progress
- Add discovered tasks as needed
- Document deviations and reasons
